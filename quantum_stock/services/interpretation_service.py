"""
Interpretation Service - LLM-based Vietnamese narrative generation for trading data
Uses OpenAI-compatible API through local LLM proxy for actionable insights
"""

import logging
import json
import os
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)


class InterpretationService:
    """
    Shared LLM interpretation service for generating Vietnamese trading narratives

    Uses claudible-haiku-4.5 (fast) and claudible-sonnet-4.6 (deep analysis)
    through local proxy at http://localhost:8317/v1
    """

    # LLM Configuration
    LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://localhost:8317/v1")
    LLM_API_KEY = os.getenv("LLM_API_KEY", "")
    MODEL_FAST = "claude-sonnet-4-6"
    MODEL_DEEP = "claude-sonnet-4-6"

    # Cache TTL
    CACHE_TTL = 300  # 5 minutes

    # Vietnamese prompt templates (max 200 words output, actionable, with emoji)
    PROMPT_TEMPLATES = {
        "market_status": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam.

Dá»¯ liá»‡u thá»‹ trÆ°á»ng:
{data}

HÃ£y tÃ³m táº¯t tá»•ng quan thá»‹ trÆ°á»ng báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- TÃ¬nh tráº¡ng VN-Index (tÄƒng/giáº£m/Ä‘i ngang)
- Äá»™ rá»™ng thá»‹ trÆ°á»ng (mÃ£ tÄƒng vs mÃ£ giáº£m)
- Khá»‘i ngoáº¡i (mua rÃ²ng/bÃ¡n rÃ²ng)
- Khuyáº¿n nghá»‹ hÃ nh Ä‘á»™ng cá»¥ thá»ƒ

DÃ¹ng emoji Ä‘á»ƒ dá»… Ä‘á»c. Káº¿t thÃºc báº±ng 1 cÃ¢u khuyáº¿n nghá»‹ rÃµ rÃ ng.""",

        "market_regime": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch xu hÆ°á»›ng thá»‹ trÆ°á»ng.

Market Regime: {regime}
Dá»¯ liá»‡u bá»• sung:
{data}

Giáº£i thÃ­ch ngáº¯n gá»n báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- Ã nghÄ©a cá»§a regime nÃ y (bull/bear/neutral/sideways)
- Äiá»u gÃ¬ sáº½ xáº£y ra tiáº¿p theo
- Chiáº¿n lÆ°á»£c giao dá»‹ch phÃ¹ há»£p
- Má»©c rá»§i ro cáº§n lÆ°u Ã½

DÃ¹ng emoji, ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng, dá»… hiá»ƒu.""",

        "smart_signals": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch tÃ­n hiá»‡u thÃ´ng minh.

TÃ­n hiá»‡u phÃ¡t hiá»‡n:
{data}

Diá»…n giáº£i tÃ­n hiá»‡u báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- Loáº¡i tÃ­n hiá»‡u (breakout, reversal, momentum, etc.)
- Äá»™ tin cáº­y cá»§a tÃ­n hiá»‡u
- HÃ nh Ä‘á»™ng cá»¥ thá»ƒ: MUA/BÃN/CHá»œ
- Stop-loss vÃ  take-profit Ä‘á» xuáº¥t

Ngáº¯n gá»n, dá»… hiá»ƒu, cÃ³ emoji, káº¿t thÃºc báº±ng khuyáº¿n nghá»‹.""",

        "technical_analysis": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch ká»¹ thuáº­t.

MÃ£ cá»• phiáº¿u: {symbol}
Indicators:
{data}

PhÃ¢n tÃ­ch ká»¹ thuáº­t báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- RSI, MACD, MA Ä‘ang nÃ³i gÃ¬?
- Xu hÆ°á»›ng hiá»‡n táº¡i (uptrend/downtrend/sideways)
- Káº¿t luáº­n rÃµ rÃ ng: MUA/BÃN/CHá»œ
- Äiá»ƒm vÃ o lá»‡nh vÃ  stop-loss

DÃ¹ng emoji, ngÃ´n ngá»¯ Ä‘Æ¡n giáº£n, káº¿t thÃºc báº±ng VERDICT rÃµ rÃ ng.""",

        "news_mood": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch sentiment tin tá»©c.

Tin tá»©c gáº§n Ä‘Ã¢y:
{data}

TÃ³m táº¯t sentiment báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- TÃ¢m lÃ½ chung (tÃ­ch cá»±c/tiÃªu cá»±c/trung tÃ­nh)
- Chá»§ Ä‘á» nÃ³ng Ä‘ang Ä‘Æ°á»£c quan tÃ¢m
- TÃ¡c Ä‘á»™ng lÃªn thá»‹ trÆ°á»ng
- Cá»• phiáº¿u nÃ o Ä‘Æ°á»£c Ä‘á» cáº­p nhiá»u

Ngáº¯n gá»n, cÃ³ emoji, káº¿t thÃºc báº±ng khuyáº¿n nghá»‹.""",

        "news_alerts": """Báº¡n lÃ  chuyÃªn gia lá»c tin tá»©c quan trá»ng.

Tin tá»©c:
{data}

TÃ³m táº¯t tin quan trá»ng báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- Top 3-5 tin nÃ³ng nháº¥t
- TÃ¡c Ä‘á»™ng ngay láº­p tá»©c lÃªn giÃ¡
- Cá»• phiáº¿u bá»‹ áº£nh hÆ°á»Ÿng
- HÃ nh Ä‘á»™ng nÃªn lÃ m

DÃ¹ng emoji, bullet points, ngáº¯n gá»n.""",

        "backtest_result": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch backtest.

Káº¿t quáº£ backtest:
{data}

PhÃ¢n tÃ­ch káº¿t quáº£ báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- Hiá»‡u suáº¥t chiáº¿n lÆ°á»£c (win rate, profit, drawdown)
- Äiá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u
- CÃ³ nÃªn sá»­ dá»¥ng chiáº¿n lÆ°á»£c nÃ y khÃ´ng?
- Äá» xuáº¥t cáº£i thiá»‡n hoáº·c Ä‘iá»u chá»‰nh tham sá»‘

DÃ¹ng emoji, ngÃ´n ngá»¯ thá»±c táº¿, káº¿t thÃºc báº±ng khuyáº¿n nghá»‹ rÃµ rÃ ng.""",

        "deep_flow": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch dÃ²ng tiá»n.

Dá»¯ liá»‡u dÃ²ng tiá»n:
{data}

Diá»…n giáº£i dÃ²ng tiá»n báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- Tiá»n Ä‘ang cháº£y vÃ o/ra khá»i thá»‹ trÆ°á»ng
- NhÃ³m ngÃ nh nÃ o Ä‘ang hÃºt tiá»n
- Smart money vs Retail money
- Cá»• phiáº¿u nÃ o Ä‘Ã¡ng chÃº Ã½

DÃ¹ng emoji, ngáº¯n gá»n, káº¿t thÃºc báº±ng danh sÃ¡ch top 3-5 mÃ£ Ä‘Ã¡ng mua.""",

        "data_stats": """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch dá»¯ liá»‡u thá»‹ trÆ°á»ng.

Dá»¯ liá»‡u há»‡ thá»‘ng:
{data}

TÃ³m táº¯t tÃ¬nh tráº¡ng dá»¯ liá»‡u báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 200 tá»«):
- Cháº¥t lÆ°á»£ng dá»¯ liá»‡u (Ä‘áº§y Ä‘á»§/thiáº¿u)
- Nguá»“n dá»¯ liá»‡u Ä‘ang hoáº¡t Ä‘á»™ng
- Khuyáº¿n nghá»‹ cáº£i thiá»‡n

DÃ¹ng emoji, ngáº¯n gá»n.""",

        "agent_chat": """Báº¡n lÃ  trá»£ lÃ½ phÃ¢n tÃ­ch chá»©ng khoÃ¡n Viá»‡t Nam thÃ´ng minh.

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {query}

Dá»¯ liá»‡u thá»‹ trÆ°á»ng hiá»‡n táº¡i:
{data}

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 300 tá»«):
- PhÃ¢n tÃ­ch dá»±a trÃªn dá»¯ liá»‡u thá»±c
- ÄÆ°a ra khuyáº¿n nghá»‹ cá»¥ thá»ƒ
- Sá»­ dá»¥ng emoji cho dá»… Ä‘á»c

Náº¿u khÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u, hÃ£y nÃ³i rÃµ vÃ  Ä‘Æ°a ra phÃ¢n tÃ­ch tá»•ng quan.""",

        "agent_analysis": """Báº¡n Ä‘ang Ä‘Ã³ng vai {role} trong team phÃ¢n tÃ­ch cá»• phiáº¿u.

Vai trÃ²: {role_description}
MÃ£ cá»• phiáº¿u: {symbol}

Dá»¯ liá»‡u ká»¹ thuáº­t:
{data}

ÄÆ°a ra phÃ¢n tÃ­ch ngáº¯n gá»n (tá»‘i Ä‘a 150 tá»«) theo gÃ³c nhÃ¬n cá»§a vai trÃ².
Káº¿t thÃºc báº±ng verdict: MUA / BÃN / CHá»œ vá»›i confidence %.
DÃ¹ng emoji."""
    }

    def __init__(self):
        """Initialize interpretation service with LLM client"""
        self.client = AsyncOpenAI(
            base_url=self.LLM_BASE_URL,
            api_key=self.LLM_API_KEY
        )
        self.cache: Dict[str, tuple[datetime, str]] = {}  # key -> (timestamp, result)
        self.enabled = True
        logger.info(f"InterpretationService initialized: {self.LLM_BASE_URL}")

    def _get_cache_key(self, template_name: str, data: Dict[str, Any]) -> str:
        """Generate cache key from template and data"""
        # Use template + hash of data for cache key
        data_str = json.dumps(data, sort_keys=True)
        return f"{template_name}:{hash(data_str)}"

    def _check_cache(self, cache_key: str) -> Optional[str]:
        """Check if cached result exists and is still valid"""
        if cache_key in self.cache:
            cached_time, cached_result = self.cache[cache_key]
            if (datetime.now() - cached_time).total_seconds() < self.CACHE_TTL:
                logger.debug(f"Cache hit for {cache_key}")
                return cached_result
        return None

    def _set_cache(self, cache_key: str, result: str):
        """Store result in cache"""
        self.cache[cache_key] = (datetime.now(), result)

        # Clean old cache entries (simple cleanup)
        if len(self.cache) > 100:
            # Remove oldest 20 entries
            sorted_keys = sorted(self.cache.keys(), key=lambda k: self.cache[k][0])
            for key in sorted_keys[:20]:
                del self.cache[key]

    async def interpret(
        self,
        template_name: str,
        data: Dict[str, Any],
        model: str = None,
        language: str = "vi"
    ) -> str:
        """
        Generate interpretation using LLM

        Args:
            template_name: Name of prompt template (market_status, market_regime, etc.)
            data: Data to interpret
            model: Model to use (default: MODEL_FAST)
            language: Output language (only 'vi' supported for now)

        Returns:
            Vietnamese interpretation text (max 200 words)
        """
        if not self.enabled:
            return f"[Interpretation service chÆ°a sáºµn sÃ ng]"

        # Check cache
        cache_key = self._get_cache_key(template_name, data)
        cached = self._check_cache(cache_key)
        if cached:
            return cached

        # Get prompt template
        if template_name not in self.PROMPT_TEMPLATES:
            logger.warning(f"Unknown template: {template_name}")
            return f"[Template '{template_name}' khÃ´ng tá»“n táº¡i]"

        prompt_template = self.PROMPT_TEMPLATES[template_name]

        # Format prompt with data
        try:
            # Convert data to readable format
            data_str = json.dumps(data, indent=2, ensure_ascii=False)

            # Special handling for different templates
            if template_name == "technical_analysis" and "symbol" in data:
                prompt = prompt_template.format(
                    symbol=data.get("symbol", "N/A"),
                    data=data_str
                )
            elif template_name == "market_regime" and "regime" in data:
                prompt = prompt_template.format(
                    regime=data.get("regime", "N/A"),
                    data=data_str
                )
            elif template_name == "agent_chat":
                prompt = prompt_template.format(
                    query=data.get("query", "N/A"),
                    data=data_str
                )
            elif template_name == "agent_analysis":
                prompt = prompt_template.format(
                    role=data.get("role", "N/A"),
                    role_description=data.get("role_description", "N/A"),
                    symbol=data.get("symbol", "N/A"),
                    data=data_str
                )
            else:
                prompt = prompt_template.replace("{data}", data_str)

        except Exception as e:
            logger.error(f"Failed to format prompt: {e}")
            return f"[Lá»—i format prompt: {e}]"

        # Call LLM
        try:
            model_to_use = model or self.MODEL_FAST

            response = await self.client.chat.completions.create(
                model=model_to_use,
                messages=[
                    {
                        "role": "system",
                        "content": "Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam. Tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch, cÃ³ emoji, tá»‘i Ä‘a 200 tá»«."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=500
            )

            result = response.choices[0].message.content.strip()

            # Cache result
            self._set_cache(cache_key, result)

            logger.info(f"âœ… LLM interpretation: {template_name} ({len(result)} chars)")
            return result

        except Exception as e:
            logger.error(f"LLM API error: {e}")
            # Return fallback message
            return self._fallback_interpretation(template_name, data)

    def _fallback_interpretation(self, template_name: str, data: Dict[str, Any]) -> str:
        """Fallback interpretation when LLM fails"""
        fallbacks = {
            "market_status": "ðŸ“Š Thá»‹ trÆ°á»ng Ä‘ang Ä‘Æ°á»£c phÃ¢n tÃ­ch. Vui lÃ²ng thá»­ láº¡i sau.",
            "market_regime": "ðŸ“ˆ Xu hÆ°á»›ng thá»‹ trÆ°á»ng: Dá»¯ liá»‡u Ä‘ang Ä‘Æ°á»£c xá»­ lÃ½.",
            "smart_signals": "ðŸŽ¯ TÃ­n hiá»‡u giao dá»‹ch Ä‘ang Ä‘Æ°á»£c phÃ¢n tÃ­ch.",
            "technical_analysis": "ðŸ“‰ PhÃ¢n tÃ­ch ká»¹ thuáº­t: Äang káº¿t ná»‘i LLM service.",
            "news_mood": "ðŸ“° Sentiment tin tá»©c: Äang xá»­ lÃ½.",
            "news_alerts": "ðŸš¨ Tin tá»©c quan trá»ng: Äang tá»•ng há»£p.",
            "backtest_result": "ðŸ”¬ Káº¿t quáº£ backtest: Äang phÃ¢n tÃ­ch.",
            "deep_flow": "ðŸ’° DÃ²ng tiá»n: Äang theo dÃµi.",
            "agent_chat": "ðŸ¤– Trá»£ lÃ½ AI Ä‘ang xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n. Vui lÃ²ng thá»­ láº¡i sau.",
            "agent_analysis": "ðŸ“Š Agent Ä‘ang phÃ¢n tÃ­ch. LLM service táº¡m thá»i khÃ´ng kháº£ dá»¥ng."
        }
        return fallbacks.get(template_name, "â³ Äang xá»­ lÃ½ dá»¯ liá»‡u...")

    async def batch_interpret(
        self,
        items: List[tuple[str, Dict[str, Any]]],
        model: str = None
    ) -> List[str]:
        """
        Batch interpretation for multiple items

        Args:
            items: List of (template_name, data) tuples
            model: Model to use for all items

        Returns:
            List of interpretation strings
        """
        results = []
        for template_name, data in items:
            result = await self.interpret(template_name, data, model=model)
            results.append(result)
        return results


# Singleton instance
_interpretation_service: Optional[InterpretationService] = None


def get_interpretation_service() -> InterpretationService:
    """Get or create interpretation service singleton"""
    global _interpretation_service
    if _interpretation_service is None:
        _interpretation_service = InterpretationService()
    return _interpretation_service
